{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ubuntu/spark/\" #/spark-3.3.2-bin-hadoop3.2.1\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/usr/local/hadoop/etc/hadoop/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.appName(\"Sentiment140\").master('yarn') \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Disable logging\n",
    "import logging\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\").setLevel(logger.Level.OFF)\n",
    "logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.OFF)\n",
    "\n",
    "spark.conf.set(\"spark.driver.log.level\", \"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"label\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"/dis_materials/csv_output1\", header=False, schema=schema)\n",
    "\n",
    "# Transform text variables to proper types\n",
    "# df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"EEE MMM dd HH:mm:ss z yyyy\"))\n",
    "\n",
    "# Cache the DataFrame\n",
    "df.cache() # Caching intermediate DataFrames - Optimization1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows and columns\n",
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(\"There are {:,} rows and {} columns.\\n\".format(num_rows, num_cols))\n",
    "\n",
    "# Print First 5 Rows\n",
    "df.show(5)\n",
    "\n",
    "# Show descriptive statistics for selected columns\n",
    "df.select('label', 'timestamp', 'query', 'text').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "# Tokenize the 'text' column\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(df)\n",
    "\n",
    "# Calculate the term frequency (TF)\n",
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"raw_features\")\n",
    "tf_data = hashing_tf.transform(words_data)\n",
    "\n",
    "# Calculate the inverse document frequency (IDF)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(tf_data)\n",
    "tfidf_data = idf_model.transform(tf_data)\n",
    "\n",
    "tfidf_data.show(truncate=False)\n",
    "\n",
    "# Set the number of output partitions\n",
    "tfidf_data = tfidf_data.coalesce(10) #Adjusting the number of output partitions - optimization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results (including TF-IDF features) to a Delta table\n",
    "delta_table_path = \"hdfs:///dis_materials/group15_delta\"\n",
    "tfidf_data.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(delta_table_path)\n",
    "\n",
    "# Merge the new data with the existing Delta table to remove duplicates\n",
    "from delta.tables import DeltaTable\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "delta_table.alias(\"oldData\") \\\n",
    "    .merge(tfidf_data.alias(\"newData\"), \"oldData.id = newData.id\") \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
